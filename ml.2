import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt       # matplotlib.pyplot plots data
%matplotlib inline
import seaborn as sns

pdata = pd.read_csv("/content/archive (1).zip")

pdata.shape # Check number of columns and rows in data frame

pdata.head() # To check first 5 rows of data set

pdata.isnull().values.any() # If there are any null values in data set

pdata.tail()

columns = list(pdata)[0:-1] # Excluding Outcome column which has only
pdata[columns].hist(stacked=False, bins=100, figsize=(12,30), layout=(14,2));
# Histogram of first 8 columns


pdata.info()

pdata.corr() # It will show correlation matrix

plot_corr(pdata)

sns.pairplot(pdata,diag_kind='kde')

n_true = len(pdata.loc[pdata['Family size'] == 1])  # Assuming 1 represents True
n_false = len(pdata.loc[pdata['Family size'] == 0])  # Assuming 0 represents False
print("Number of true cases: {0} ({1:2.2f}%)".format(n_true, (n_true / (n_true + n_false)) * 100 ))
print("Number of false cases: {0} ({1:2.2f}%)".format(n_false, (n_false / (n_true + n_false)) * 100))


from sklearn.model_selection import train_test_split

X = pdata.drop('Pin code',axis=1)     # Predictor feature columns (8 X m)
Y = pdata['Pin code']   # Predicted class (1=True, 0=False) (1 X m)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)
# 1 is just any random seed number

x_train.head()

print("{0:0.2f}% data is in training set".format((len(x_train)/len(pdata.index)) * 100))
print("{0:0.2f}% data is in test set".format((len(x_test)/len(pdata.index)) * 100))

x_train.head()

x_train.info()


from sklearn.impute import SimpleImputer
import pandas as pd

# Define non-numeric columns
non_numeric_cols = x_train.select_dtypes(exclude=["int64", "float64"]).columns

# Impute missing values in x_test for non-numeric columns with most frequent value
rep_1 = SimpleImputer(strategy="most_frequent")
x_test_non_numeric = pd.DataFrame(rep_1.fit_transform(x_test[non_numeric_cols]), columns=non_numeric_cols)

# Impute missing values in x_test for numeric columns with mean value
numeric_cols = x_test.select_dtypes(include=["int64", "float64"]).columns
rep_0 = SimpleImputer(strategy="mean")
x_test_numeric = pd.DataFrame(rep_0.fit_transform(x_test[numeric_cols]), columns=numeric_cols)

# Concatenate imputed non-numeric and numeric columns
x_test_imputed = pd.concat([x_test_non_numeric, x_test_numeric], axis=1)

# Reassign column names
x_test_imputed.columns = x_train.columns

# Display head of x_train
print(x_test_imputed.head())


from sklearn import metrics
from sklearn.linear_model import LogisticRegression
import pandas as pd

# Check the shape of the dataframes
print("Shape of x_train:", x_train.shape)
print("Shape of x_test:", x_test.shape)
print("Shape of y_train:", y_train.shape)

# Fit the model on train
model = LogisticRegression()
model.fit(x_train,y_train)

# Predict on test
y_predict = model.predict(x_test)

# Check the coefficients
if x_train.shape[1] == len(model.coef_[0]):
    coef_df = pd.DataFrame(model.coef_)
    coef_df['intercept'] = model.intercept_
    print(coef_df)
else:
    print("Number of features in x_train does not match the number of coefficients:")



model_score = model.score(x_test, y_test)
print("model_score:",model_score)

cm=metrics.confusion_matrix(y_test,y_pred, labels=[1, 0])

df_cm = pd.DataFrame(cm, index = [i for i in ["1","0"]],
                  columns = [i for i in ["Predict 1","Predict 0"]])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True)
